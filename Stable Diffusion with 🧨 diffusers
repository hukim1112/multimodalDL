{"cells":[{"cell_type":"markdown","metadata":{"id":"gd-vX3cavOCt"},"source":["# **Stable Diffusion** 🎨 \n","*...using `🧨diffusers`*\n","\n","Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM.\n","See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n","\n","This Colab notebook shows how to use Stable Diffusion with the 🤗 Hugging Face [🧨 Diffusers library](https://github.com/huggingface/diffusers). \n","\n","Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"-xMJ6LaET6dT"},"source":["## 1. How to use `StableDiffusionPipeline`\n","\n","Before diving into the theoretical aspects of how Stable Diffusion functions, \n","let's try it out a bit 🤗.\n","\n","In this section, we show how you can run text to image inference in just a few lines of code!"]},{"cell_type":"markdown","metadata":{"id":"QYOlvQ1nQL7c"},"source":["### Setup\n","\n","First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHkHsdtnry57"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"paJt_cx5QgVz"},"source":["Next, you should install `diffusers==0.4.0` as well `scipy`, `ftfy` and `transformers`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIrgth7sqFML"},"outputs":[],"source":["!pip install diffusers===0.11.1\n","!pip install transformers scipy ftfy\n","!pip install \"ipywidgets>=7,<8\"\n","!pip install fifty accelerate"]},{"cell_type":"markdown","metadata":{"id":"3NnPOMAqAABv"},"source":["### Stable Diffusion Pipeline\n","\n","`StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.\n","\n","First, we load the pre-trained weights of all components of the model.\n","\n","## Prompt Engineering 🎨\n","\n","When running *Stable Diffusion* in inference, we usually want to generate a certain type, style of image and then improve upon it. Improving upon a previously generated image means running inference over and over again with a different prompt and potentially a different seed until we are happy with our generation. \n","\n","So to begin with, it is most important to speed up stable diffusion as much as possible to generate as many pictures as possible in a given amount of time. \n","\n","This can be done by both improving the **computational efficiency** (speed) and the **memory efficiency** (GPU RAM).\n","\n","Let's start by looking into the computational efficiency first. \n","\n","Through-out the notebook, we will focus on [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5):\n","\n","\n","### Quality Improvements\n","\n","Now that our image generation pipeline is blazing fast, let's try to get maximum image quality.\n","\n","First of all, image quality is extremely subjective, so it's difficult to make general claims here.\n","\n","The most obvious step to take to improve quality is to use *better checkpoints*. Since the release of Stable Diffusion many improved versions have been released, which are summarized here:\n","\n","- *Official Release - 22 Aug 2022*: [Stable-Diffusion 1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n","- *20 October 2022*: [Stable-Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n","- *24 Nov 2022*: [Stable-Diffusion 2.0](https://huggingface.co/stabilityai/stable-diffusion-2-0)\n","- *7 Dec 2022*: [Stable-Diffusion 2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1)\n","\n","Newer versions don't necessarily mean better image quality with the same parameters. People mentioned that *2.0* is slighly worse than *1.5* for certain prompts, but given the right prompt engineering *2.0* and *2.1* seem to be better. \n","\n","Overall, we strongly recommend to just try the models out and read up on advice online $^{2}$\n","\n","Additionally, the community has started fine-tuning many of the above versions on certain styles with some of them having an extremely high quality and gaining a lot of traction. \n","\n","We recommend to simply have a look at all [diffusers checkpoints sorted by downloads and try out the different checkpoints](https://huggingface.co/models?library=diffusers).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSKWBKFPArKS"},"outputs":[],"source":["import torch\n","from diffusers import StableDiffusionPipeline\n","model_id = \"runwayml/stable-diffusion-v1-5\"\n","pipe = StableDiffusionPipeline.from_pretrained(model_id)  "]},{"cell_type":"markdown","metadata":{"id":"8MgNzTxwbASv"},"source":["Next, let's move the pipeline to GPU to have faster inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LA9myHTxbDhm"},"outputs":[],"source":["pipe = pipe.to(\"cuda\")"]},{"cell_type":"markdown","source":["To make sure we can reproduce more or less the same image in every call, let's make use of the generator. See documentation on reproducibality [here]( ) for more information."],"metadata":{"id":"SgD2A3I_X2sG"}},{"cell_type":"code","source":["import torch\n","\n","generator = torch.Generator(\"cuda\").manual_seed(0)"],"metadata":{"id":"YbItI21CYIax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To generate an image, you should use the `__call__` method. You can have a look at its docstring to better understand what arguments can be passed [here](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.__call__). "],"metadata":{"id":"dVI0w37vYU6i"}},{"cell_type":"code","source":["prompt = \"portrait photo of a old warrior chief\"\n","\n","image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n","\n","# Now to display an image you can do either save it such as:\n","image.save(f\"warrior chief.png\")\n","\n","# or if you're in a google colab you can directly display it with \n","image"],"metadata":{"id":"pPmCQd9ON09O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cool, this now took roughly 30 seconds on a T4 GPU (you might see faster inference if you're allocated GPU is better than a T4).\n","\n","The default run we did above used full float32 precision and run the default 50 inference steps. The easiest speed-ups come from switiching to float16 (or half) precision and simply running less inference steps. Let's load the model now instead in float16."],"metadata":{"id":"lPV0-f3uYc51"}},{"cell_type":"code","source":["pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipe = pipe.to(\"cuda\")"],"metadata":{"id":"M7ky8BgKZPKX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And we can again call the pipeline to generate an image."],"metadata":{"id":"WLAnZx3QZROj"}},{"cell_type":"code","source":["generator = torch.Generator(\"cuda\").manual_seed(0)\n","\n","image = pipe(prompt, generator=generator).images[0] \n","image"],"metadata":{"id":"7cZxoB4bZSQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cool, this is almost three times as fast for arguably the exact same image quality.\n","\n","We strongly suggest to always run your pipelines in float16 as so far we have very rarely seen degradations in quality because of it.\n","\n","Next, let's see if we really need to use 50 inference steps or whether we could use significantly less.\n","\n","Let's have a look at all schedulers, the stable diffusion pipeline is compatible with."],"metadata":{"id":"4NCD3sDSZdSS"}},{"cell_type":"code","source":["pipe.scheduler.compatibles"],"metadata":{"id":"Z9dAIXGNZSnO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cool, that's a lot of schedulers.\n","\n","🧨 Diffusers is constantely adding a bunch of novel schedulers/samplers that can be used with Stable Diffusion. For more information, we recommend to take a look at the official documentation [here](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview).\n","\n","Alright, right now Stable Diffusion is using the `PNDMScheduler` which usually requires around 50 inference steps. However, other schedulers such as `DPMSolverMultistepScheduler` or `DPMSolverSinglestepScheduler` seem to get away with just 20 to 25 inference steps. Let's try them out. \n","\n","You can set a new scheduler by making use of the [from_config](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) function."],"metadata":{"id":"tshm4ekkZhHJ"}},{"cell_type":"code","source":["from diffusers import DPMSolverMultistepScheduler\n","\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"],"metadata":{"id":"chjvrWqSZSpp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's try to reduce the number of inference steps to just 20."],"metadata":{"id":"_NIj9OmdZktI"}},{"cell_type":"code","source":["generator = torch.Generator(\"cuda\").manual_seed(0)\n","\n","image = pipe(prompt, generator=generator, num_inference_steps=20).images[0] \n","image"],"metadata":{"id":"7Gexn9WjZk_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cool, let's add more details."],"metadata":{"id":"L-HtuO8WPu9L"}},{"cell_type":"code","source":["prompt = \"portrait photo of a old warrior chief\"\n","prompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\""],"metadata":{"id":"aniFMWiuN1Bn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt"],"metadata":{"id":"SVjgUtsvTDgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and let's also add some cues that usually help to generate higher quality images."],"metadata":{"id":"5Mgg7LvmPyhB"}},{"cell_type":"code","source":["prompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\"\n","prompt"],"metadata":{"id":"q36eyVeuPugA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["let's now try this prompt."],"metadata":{"id":"7oJew1DEP4WT"}},{"cell_type":"code","source":["image = pipe(prompt, generator=generator, num_inference_steps=20).images[0]\n","image"],"metadata":{"id":"zpUVTZiVTbaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","\n","def image_grid(imgs, rows, cols):\n","    assert len(imgs) == rows*cols\n","\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","    \n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid"],"metadata":{"id":"BnCz5PWiozvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_cols = 3\n","num_rows = 4\n","\n","prompt = [prompt] * num_cols\n","all_images = []\n","for i in range(num_rows):\n","  images = pipe(prompt, generator=generator, num_inference_steps=20).images\n","  all_images.extend(images)\n","\n","grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n","grid"],"metadata":{"id":"s5K46GpZP36J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try another prompt."],"metadata":{"id":"9WWhJJSqOxAr"}},{"cell_type":"code","source":["prompt = \"a photo of an astronaut riding a horse on mars\"\n","image =pipe(prompt, generator=generator, num_inference_steps=20).images[0]\n","    \n","image.save(\"astronaut_rides_horse.png\")\n","\n","image"],"metadata":{"id":"JptUxDbkN06e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXZRu7PfvTwN"},"outputs":[],"source":["prompt = 'some knights riding horses in an beautiful ancient forest.'\n","image = pipe(prompt, generator=generator, num_inference_steps=20).images[0] \n","\n","# Now to display an image you can do either save it such as:\n","image.save(f\"knights.png\")\n","\n","# or if you're in a google colab you can directly display it with \n","image"]},{"cell_type":"markdown","metadata":{"id":"Y8wxFjba5zRc"},"source":["The other parameter in the pipeline call is `guidance_scale`. It is a way to increase the adherence to the conditional signal which in this case is text as well as overall sample quality. In simple terms classifier free guidance forces the generation to better match with the prompt. Numbers like `7` or `8.5` give good results, if you use a very large number the images might look good, but will be less diverse. \n","\n","You can learn about the technical details of this parameter in [the last section](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S) of this notebook."]},{"cell_type":"markdown","metadata":{"id":"uf9pbS3kCsUf"},"source":["### Generate non-square images\n","\n","Stable Diffusion produces images of `512 × 512` pixels by default. But it's very easy to override the default using the `height` and `width` arguments, so you can create rectangular images in portrait or landscape ratios.\n","\n","These are some recommendations to choose good image sizes:\n","- Make sure `height` and `width` are both multiples of `8`.\n","- Going below 512 might result in lower quality images.\n","- Going over 512 in both directions will repeat image areas (global coherence is lost).\n","- The best way to create non-square images is to use `512` in one dimension, and a value larger than that in the other one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SXnxd-ZrSfy"},"outputs":[],"source":["prompt = \"a photograph of an astronaut riding a horse\"\n","\n","image = pipe(prompt, height=512, width=768).images[0]\n","image"]},{"cell_type":"markdown","metadata":{"id":"yW14FA-tDQ5n"},"source":["## 2. What is Stable Diffusion\n","\n","Now, let's go into the theoretical part of Stable Diffusion 👩‍🎓.\n","\n","Stable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zHj_sllMaKTD"},"source":["General diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).\n","\n","Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference."]},{"cell_type":"markdown","metadata":{"id":"BBsdAj9pDPOv"},"source":["\n","\n","<br>\n","\n","Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.** \n","\n","There are three main components in latent diffusion.\n","\n","1. An autoencoder (VAE).\n","2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n","3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."]},{"cell_type":"markdown","metadata":{"id":"j4leRMZzjTsA"},"source":["**1. The autoencoder (VAE)**\n","\n","The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n","The decoder, conversely, transforms the latent representation back into an image.\n","\n"," During latent diffusion _training_, the encoder is used to get the latent representations (_latents_) of the images for the forward diffusion process, which applies more and more noise at each step. During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As we will see during inference we **only need the VAE decoder**."]},{"cell_type":"markdown","metadata":{"id":"Jr5ZCb66kmyE"},"source":["**2. The U-Net**\n","\n","The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.\n","The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\n","More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation.\n","\n","To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n","Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks."]},{"cell_type":"markdown","metadata":{"id":"YE7hhg5ArUu4"},"source":["**3. The Text-encoder**\n","\n","The text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronout riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.\n","\n","Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."]},{"cell_type":"markdown","metadata":{"id":"2-XnKTVfj2Jm"},"source":["**Why is latent diffusion fast and efficient?**\n","\n","Since the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8. This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 × 8 = 64` times less memory.\n","\n","This is why it's possible to generate `512 × 512` images so quickly, even on 16GB Colab GPUs!"]},{"cell_type":"markdown","metadata":{"id":"Zz5Ge_47jUaA"},"source":["**Stable Diffusion during inference**\n","\n","Putting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow.\n"]},{"cell_type":"markdown","metadata":{"id":"cUBqX1sMsDR6"},"source":["<p align=\"left\">\n","<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n","</p>\n","\n","The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size $64 \\times 64$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.\n","\n","Next the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n","\n","- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py) (used by default)\n","- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n","- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)\n","\n","Theory on how the scheduler algorithm function is out of scope for this notebook, but in short one should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n","For more information, we recommend looking into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n","\n","The *denoising* process is repeated *ca.* 50 times to step-by-step retrieve better latent image representations.\n","Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder."]},{"cell_type":"markdown","metadata":{"id":"2rR2Udg5IYjn"},"source":["\n","\n","After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of 🤗 Hugging Face Diffusers!"]},{"cell_type":"markdown","metadata":{"id":"UZp-ynZLrS-S"},"source":["## 3. How to write your own inference pipeline with `diffusers`\n","\n","Finally, we show how you can create custom diffusion pipelines with `diffusers`.\n","This is often very useful to dig a bit deeper into certain functionalities of the system and to potentially switch out certain components. \n","\n","In this section, we will demonstrate how to use Stable Diffusion with a different scheduler, namely [Katherine Crowson's](https://github.com/crowsonkb) K-LMS scheduler that was added in [this PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365)."]},{"cell_type":"markdown","metadata":{"id":"KEXkmX3vDPRU"},"source":["Let's go through the `StableDiffusionPipeline` step by step to see how we could have written it ourselves.\n","\n","We will start by loading the individual models involved."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA-VoQm3YW-5"},"outputs":[],"source":["import torch\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"n3Xw7qSEdTpt"},"source":["The [pre-trained model](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders:\n","- `text_encoder`: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as `BERT`.\n","- `tokenizer`. It must match the one used by the `text_encoder` model.\n","- `scheduler`: The scheduling algorithm used to progressively add noise to the image during training.\n","- `unet`: The model used to generate the latent representation of the input.\n","- `vae`: Autoencoder module that we'll use to decode latent representations into real images.\n","\n","We can load the components by referring to the folder they were saved, using the `subfolder` argument to `from_pretrained`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlsKwQijWMpL"},"outputs":[],"source":["from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n","\n","# 1. Load the autoencoder model which will be used to decode the latents into image space. \n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n","\n","# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n","\n","# 3. The UNet model for generating the latents.\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"]},{"cell_type":"markdown","metadata":{"id":"8eYstOqwVYGc"},"source":["Now instead of loading the pre-defined scheduler, we load a K-LMS scheduler instead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-0W8UG6VXpD"},"outputs":[],"source":["from diffusers import LMSDiscreteScheduler\n","\n","scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"]},{"cell_type":"markdown","metadata":{"id":"yBXqwuHFYgf4"},"source":["Next we move the models to the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3lJEXz7YgnC"},"outputs":[],"source":["vae = vae.to(torch_device)\n","text_encoder = text_encoder.to(torch_device)\n","unet = unet.to(torch_device) "]},{"cell_type":"markdown","metadata":{"id":"YtqGYl5SY6dm"},"source":["We now define the parameters we'll use to generate images.\n","\n","Note that `guidance_scale` is defined analog to the guidance weight `w` of equation (2) in the [Imagen paper](https://arxiv.org/pdf/2205.11487.pdf). `guidance_scale == 1` corresponds to doing no classifier-free guidance. Here we set it to 7.5 as also done previously.\n","\n","In contrast to the previous examples, we set `num_inference_steps` to 100 to get an even more defined image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ot8RDV-2Y6uE"},"outputs":[],"source":["prompt = [\"a photograph of an astronaut riding a horse\"]\n","\n","height = 512                        # default height of Stable Diffusion\n","width = 512                         # default width of Stable Diffusion\n","\n","num_inference_steps = 100            # Number of denoising steps\n","\n","guidance_scale = 7.5                # Scale for classifier-free guidance\n","\n","generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n","\n","batch_size = 1"]},{"cell_type":"markdown","metadata":{"id":"G47gEbg9Z4sJ"},"source":["First, we get the text_embeddings for the prompt. These embeddings will be used to condition the UNet model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZpvyVT1Y6wq"},"outputs":[],"source":["text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sMQDtnGxQDC"},"outputs":[],"source":["text_embeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"INGdc9eFaeWz"},"source":["We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional `text_embeddings` (`batch_size` and `seq_length`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkuwhbFrY6zo"},"outputs":[],"source":["max_length = text_input.input_ids.shape[-1]\n","uncond_input = tokenizer(\n","    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",")\n","with torch.no_grad():\n","  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdRBSz6SxRtH"},"outputs":[],"source":["uncond_embeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"3lKMrvoYbxzf"},"source":["For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwnB7CIeY619"},"outputs":[],"source":["text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iShKyRA3xWCC"},"outputs":[],"source":["text_embeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"oxcaMgD0DPUD"},"source":["Generate the intial random noise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NsfjxA-chAL"},"outputs":[],"source":["latents = torch.randn(\n","  (batch_size, unet.in_channels, height // 8, width // 8),\n","  generator=generator,\n",")\n","latents = latents.to(torch_device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nC401krZfXOr"},"outputs":[],"source":["latents.shape"]},{"cell_type":"markdown","metadata":{"id":"JDUOA1gHMp2Y"},"source":["Cool $64 \\times 64$ is expected. The model will transform this latent representation (pure noise) into a `512 × 512` image later on.\n","\n","Next, we initialize the scheduler with our chosen `num_inference_steps`.\n","This will compute the `sigmas` and exact time step values to be used during the denoising process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6UDqCyKwBpx"},"outputs":[],"source":["scheduler.set_timesteps(num_inference_steps)"]},{"cell_type":"markdown","metadata":{"id":"xTOxOKeqW4XE"},"source":["The K-LMS scheduler needs to multiply the `latents` by its `sigma` values. Let's do this here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTwTq9d-W_NP"},"outputs":[],"source":["latents = latents * scheduler.init_noise_sigma"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8t_V4E0yLI2"},"outputs":[],"source":["latents.shape"]},{"cell_type":"markdown","metadata":{"id":"LdVkvYuYdjc6"},"source":["We are ready to write the denoising loop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ylc3AIdZkFhl"},"outputs":[],"source":["from tqdm.auto import tqdm\n","from torch import autocast\n","\n","for t in tqdm(scheduler.timesteps):\n","  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n","  latent_model_input = torch.cat([latents] * 2)\n","\n","  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n","\n","  # predict the noise residual\n","  with torch.no_grad():\n","    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","\n","  # perform guidance\n","  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","\n","  # compute the previous noisy sample x_t -> x_t-1\n","  latents = scheduler.step(noise_pred, t, latents).prev_sample"]},{"cell_type":"code","source":["latents.shape"],"metadata":{"id":"q3N7-s1ndQYi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZUTKjm0kuDY"},"source":["We now use the `vae` to decode the generated `latents` back into the image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YRzuJP7kMZo"},"outputs":[],"source":["# scale and decode the image latents with vae\n","latents = 1 / 0.18215 * latents\n","\n","with torch.no_grad():\n","  image = vae.decode(latents).sample"]},{"cell_type":"markdown","metadata":{"id":"PmdOa4_Dqrl8"},"source":["And finally, let's convert the image to PIL so we can display or save it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAVZStIokTVv"},"outputs":[],"source":["image = (image / 2 + 0.5).clamp(0, 1)\n","image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","images = (image * 255).round().astype(\"uint8\")\n","pil_images = [Image.fromarray(image) for image in images]\n","pil_images[0]"]},{"cell_type":"code","source":["images.shape"],"metadata":{"id":"Rztdmwu8cffC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjYuICyxwpeO"},"source":["Nice, as we can see the photo has quite a definition 🔥."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMcqB6iNH8sT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1X_V79XrrzVr8WJUsY7M41-6OFNFLjOVd","timestamp":1680561370555},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1670284599767}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}